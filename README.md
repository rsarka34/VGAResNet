# VGAResNet: A Unified Visibility Graph Adjacency Matrix-Based Residual Network for Chronic Obstructive Pulmonary Disease Detection Using Lung Sounds
Authors: Arka Roy, Arushi Thakur, Udit Satija, Department of Electrical Engineering, Indian Institute of Technology Patna.
# Abstract:
Chronic obstructive pulmonary disease (COPD) is one of the most severe respiratory diseases and can be diagnosed by several clinical modalities such as spirometric measures, lung function tests, parametric response mapping, wheezing events of lung sounds (LSs), etc. Since LSs are related to the respiratory irregularities caused by pulmonary illnesses, examining them is more effective for identifying respiratory issues. In this letter, we propose a visibility graph (VG)-based adjacency matrix representation of LS in conjunction with a residual deep neural network (ResNet) for accurate detection of COPD, namely, the VGAResNet. The proposed framework comprises four stages: preprocessing, visibility graph creation, adjacency matrix (AdjM) generation, and lastly, classification of these AdjMs using the ResNet architecture. The proposed framework is extensively evaluated using the publicly available LS database and outperforms the existing noteworthy research works by achieving the highest performance rates of 95.13%, 96.33%, and 94.37% for accuracy, sensitivity, and specificity, respectively.\
Dataset link: https://data.mendeley.com/datasets/jwyy9np4gv/3 
# Cite as:
@article{10288401,\
  author={Roy, Arka and Thakur, Arushi and Satija, Udit},\
  journal={IEEE Sensors Letters},\  
  title={VGAResNet: A Unified Visibility Graph Adjacency Matrix-Based Residual Network for Chronic Obstructive Pulmonary Disease Detection Using Lung Sounds},\  
  year={2023},\  
  volume={7},\  
  number={11},\  
  pages={1-4},\  
  doi={10.1109/LSENS.2023.3326118}}\
@inproceedings{roy23_interspeech,\
  author={Arka Roy and Udit Satija},\
  title={{AsthmaSCELNet: A Lightweight Supervised Contrastive Embedding Learning Framework for Asthma Classification Using Lung Sounds}},\
  year=2023,\
  booktitle={Proc. INTERSPEECH 2023},\
  pages={5431--5435},\
  doi={10.21437/Interspeech.2023-428}\
}
